{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdiffeq\n",
      "  Obtaining dependency information for torchdiffeq from https://files.pythonhosted.org/packages/84/64/85249acbac630f34cd113dca4b1a72f55d3ad4c26bc9305a27aef6049756/torchdiffeq-0.2.4-py3-none-any.whl.metadata\n",
      "  Downloading torchdiffeq-0.2.4-py3-none-any.whl.metadata (440 bytes)\n",
      "Requirement already satisfied: torch>=1.5.0 in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchdiffeq) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.4.0 in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchdiffeq) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scipy>=1.4.0->torchdiffeq) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.5.0->torchdiffeq) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.5.0->torchdiffeq) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.5.0->torchdiffeq) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.5.0->torchdiffeq) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.5.0->torchdiffeq) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.5.0->torchdiffeq) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\defaultuser0.laptop-lrb3t941\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
      "Downloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: torchdiffeq\n",
      "Successfully installed torchdiffeq-0.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\defaultuser0.LAPTOP-LRB3T941\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "# Actor network (policy)\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()  # Outputs between -1 and 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.tanh(self.fc3(x))\n",
    "\n",
    "# Critic network (Q-value function)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (torch.tensor(states, dtype=torch.float32),\n",
    "                torch.tensor(actions, dtype=torch.float32),\n",
    "                torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "                torch.tensor(next_states, dtype=torch.float32),\n",
    "                torch.tensor(dones, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Dynamics function\n",
    "def dynamics(t, y, u):\n",
    "    y1, y2 = y\n",
    "    u1, u2 = u\n",
    "    dy1dt = -(u1 + 0.5 * u1**2) * y1 + 0.5 * u2 * y2 / (y1 + y2)\n",
    "    dy2dt = u1 * y1 - 0.7 * u2 * y1\n",
    "    \n",
    "    return torch.tensor([dy1dt, dy2dt])\n",
    "def reward_function(y2):\n",
    "    return y2  # Simple reward: maximize y2\n",
    "\n",
    "\n",
    "# ODE function for the system\n",
    "def ode_func(t, y):\n",
    "    with torch.no_grad():\n",
    "        u = actor(y)  # Get actions (u1, u2) from the actor network\n",
    "    return dynamics(t, y, u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "state_size = 2\n",
    "action_size = 2\n",
    "hidden_size = 128\n",
    "lr_actor = 0.001\n",
    "lr_critic = 0.002\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "buffer_size = 100000\n",
    "batch_size = 64\n",
    "\n",
    "# Instantiate networks\n",
    "actor = Actor(state_size, hidden_size, action_size)\n",
    "actor_target = Actor(state_size, hidden_size, action_size)\n",
    "critic = Critic(state_size, action_size, hidden_size)\n",
    "critic_target = Critic(state_size, action_size, hidden_size)\n",
    "\n",
    "# Copy initial weights from actor to actor target, and critic to critic target\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "# Optimizers\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_size, batch_size)\n",
    "\n",
    "# Soft update of target network parameters\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "# Training loop\n",
    "n_episodes = 500\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = torch.tensor([1.0, 0.0], dtype=torch.float32)  # Initial state [y1, y2]\n",
    "    t = torch.linspace(0., 1., 100)\n",
    "    \n",
    "    # Solve ODE using actor policy\n",
    "    with torch.no_grad():\n",
    "        sol = odeint(ode_func, state, t, method='rk4', options={'step_size': 0.01})\n",
    "    \n",
    "    # Convert solution to usable form\n",
    "    y1_sol = sol[:, 0].detach().numpy()\n",
    "    y2_sol = sol[:, 1].detach().numpy()\n",
    "    \n",
    "    total_reward = 0\n",
    "    for i in range(len(t)):\n",
    "        next_state = sol[i].detach().numpy()\n",
    "        done = (i == len(t) - 1)\n",
    "        \n",
    "        reward = reward_function(next_state[1])  # Using y2 for reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store experience in replay buffer\n",
    "        replay_buffer.add((state.numpy(), actor(state).detach().numpy(), reward, next_state, done))\n",
    "        state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        # Training step\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample()\n",
    "            \n",
    "            # Critic update\n",
    "            next_actions = actor_target(next_states)\n",
    "            Q_targets_next = critic_target(next_states, next_actions)\n",
    "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "            Q_expected = critic(states, actions)\n",
    "            critic_loss = nn.MSELoss()(Q_expected, Q_targets.detach())\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor update\n",
    "            actions_pred = actor(states)\n",
    "            actor_loss = -critic(states, actions_pred).mean()\n",
    "            \n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # Soft update target networks\n",
    "            soft_update(critic, critic_target, tau)\n",
    "            soft_update(actor, actor_target, tau)\n",
    "    \n",
    "    reward_list.append(total_reward)\n",
    "    print(f\"Episode {episode+1}/{n_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Plot rewards over episodes\n",
    "plt.plot(reward_list)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DDPG Training Progress')\n",
    "plt.show()\n",
    "\n",
    "# Plot final states y1 and y2 after training\n",
    "y1_sol = sol[:, 0].detach().numpy()\n",
    "y2_sol = sol[:, 1].detach().numpy()\n",
    "\n",
    "plt.plot(t, y1_sol, label=r'$y_1$')\n",
    "plt.plot(t, y2_sol, label=r'$y_2$')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.legend()\n",
    "plt.title('Final States After Training')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
