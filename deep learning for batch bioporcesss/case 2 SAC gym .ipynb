{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The enviorment is the updated one, where neg reward is given first , also using SAC ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SDEEnv_train_3(gym.Env):\n",
    "    \"\"\" stochasticity added ,also tuned the rewards more, and also has a fixed episode length\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SDEEnv_train_3, self).__init__()\n",
    "        # State is [y1, y2]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Actions are [u1, u2], both in some control range\n",
    "        self.action_space = spaces.Box(low=0, high=10, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Time step for numerical integration\n",
    "        self.dt = 0.1\n",
    "        \n",
    "        # Initial values for state variables y1 and y2\n",
    "        self.state = np.array([.1, .1])\n",
    "        self.episode_length = 100000  # Maximum episode length\n",
    "        self.current_step= 0\n",
    "        \n",
    "    def reset(self,seed = None,options = None):\n",
    "        # Reset the state to initial values\n",
    "        self.state = np.array([0.1, 0.1])\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        u1, u2 = action\n",
    "        y1, y2 = self.state\n",
    "        \n",
    "        dt = self.dt\n",
    "        \n",
    "        # Deterministic part of the system (first equation)\n",
    "        dy1 = ( -1*(u1 + 0.5 * u1**2 )* y1 + 0.5 * u2 * y2 / (y1 + y2) ) * dt\n",
    "        \n",
    "        # Stochastic part of the second equation\n",
    "        dW = np.random.normal(0, np.sqrt(dt))  # Wiener process for stochastic term\n",
    "        dy2 = (u1 * y1 - 0.7 * u2 * y1) * dt  #+ 0*(0.1 * np.sqrt(y1) ) * dW\n",
    "        \n",
    "        \n",
    "        # Update states\n",
    "        y1 += dy1\n",
    "        y2 += dy2\n",
    "        \n",
    "        # Ensure non-negative concentrations\n",
    "        #y1 = max(0, y1)\n",
    "        #y2 = max(0, y2)\n",
    "        \n",
    "        self.state = np.array([y1, y2])\n",
    "        \n",
    "        # Reward is based on maximizing y2\n",
    "        if y2<0.2:\n",
    "            reward= -1 + y2*5\n",
    "        else:\n",
    "            reward = y2*5       \n",
    "        # Done if the system has run too long or if values go out of bounds\n",
    "        done = False\n",
    "        \n",
    "        if y1 < 0 or y2 < 0:\n",
    "            reward = -100\n",
    "            done = True\n",
    "            \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step >= self.episode_length:\n",
    "            done = True\n",
    "        \n",
    "        #so terminate when exceeds episode length \n",
    "        \n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # Optional rendering for visualization, not essential\n",
    "        print(f\"State: y1={self.state[0]}, y2={self.state[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=SDEEnv_train_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sac_model_v2 = SAC(\n",
    "    \"MlpPolicy\", env, \n",
    "    learning_rate=3e-4,       # Start with default, reduce if needed\n",
    "    buffer_size=int(1e6),          # Large buffer for off-policy learning\n",
    "    batch_size=512,           # Larger batch for stable learning\n",
    "    tau=0.005,                # Target smoothing\n",
    "    gamma=0.90,               # Focus on long-term rewards\n",
    "    train_freq=5,            # Update less frequently for stability\n",
    "    gradient_steps=3,         # More steps to stabilize learning\n",
    "    ent_coef='auto',          # Automatically adjust entropy\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3b5e47f3d547e5ae6c01f4d0c0d50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df629c356ab74f77a4e7d427b6891de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "Sac_model_v2.learn(total_timesteps=1000000, log_interval=4, tb_log_name=\"sac\",progress_bar=True)\n",
    "Sac_model_v2.save(\"sac_mode_V3_NONSTPCHASTOCITY\")\n",
    "Sac_model_v2.save_replay_buffer(\"sac_replay_buffer_V3_NONSTPCHASTOCITY\")\n",
    "Sac_model_v2.learn(total_timesteps=500000, log_interval=4, tb_log_name=\"sac\",progress_bar=True)\n",
    "Sac_model_v2.save(\"sac_mode_V4_NONSTPCHASTOCITY\")\n",
    "Sac_model_v2.save_replay_buffer(\"sac_replay_buffer_V4_NONSTPCHASTOCITY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
