{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training model on the notreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# from sb3_contrib import  RecurrentPPO\n",
    "# unreal_env=Notreal()\n",
    "# model=RecurrentPPO(\"MlpLstmPolicy\",env= unreal_env\n",
    "#                    ,learning_rate=2*3e-4 , batch_size=256 ,n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=1000000,progress_bar=True)\n",
    "# model.save(\"model_notreal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#with added distrubences as stated in the paper, recheck the equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=1000000,progress_bar=True)\n",
    "# model.save(\"model_notreal2\")\n",
    "# model.learn(total_timesteps=1000000,progress_bar=True)\n",
    "# model.save(\"model_notreal3\")\n",
    "# model.learn(total_timesteps=1000000,progress_bar=True)\n",
    "# model.save(\"model_notreal4\")\n",
    "# model.learn(total_timesteps=1000000,progress_bar=True)\n",
    "# model.save(\"model_notreal5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    }
   ],
   "source": [
    "enviorment = Notreal(render_mode=\"human\")\n",
    "print(enviorment.render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"log.txt\", 'a') as f:\n",
    "    f.write(\"Hello, World!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Current state: [  3.65840889 824.70424258   0.        ] Total_Reward=-1000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m render_callback \u001b[38;5;241m=\u001b[39m RenderCallback(render_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model with the render callback\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mmodel_SAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m model_SAC\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC_Case 3 (REward changed).zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# model_SAC.learn(total_timesteps=500000,progress_bar=True)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# model_SAC.save(\"SAC_Case 3 V2.zip\")\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# model_SAC.learn(total_timesteps=500000,progress_bar=True)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# model_SAC.save(\"SAC_Case 3 V3.zip\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\sac\\sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[0;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\sac\\sac.py:273\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m q_values_pi \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_pi\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    274\u001b[0m min_qf_pi, _ \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mmin(q_values_pi, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    275\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (ent_coef \u001b[38;5;241m*\u001b[39m log_prob \u001b[38;5;241m-\u001b[39m min_qf_pi)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\policies.py:977\u001b[0m, in \u001b[0;36mContinuousCritic.forward\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    975\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[0;32m    976\u001b[0m qvalue_input \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([features, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(q_net(qvalue_input) \u001b[38;5;28;01mfor\u001b[39;00m q_net \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_networks)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\policies.py:977\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    975\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[0;32m    976\u001b[0m qvalue_input \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([features, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqvalue_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m q_net \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_networks)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# class RenderCallback(BaseCallback):\n",
    "#     \"\"\"\n",
    "#     A custom callback that renders the environment during training.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, render_freq=1000, verbose=0):\n",
    "#         super(RenderCallback, self).__init__(verbose)\n",
    "#         self.render_freq = render_freq\n",
    "\n",
    "#     def _on_step(self) -> bool:\n",
    "#         if self.n_calls % self.render_freq == 0:\n",
    "#             self.training_env.render()\n",
    "#         return True\n",
    "\n",
    "class RenderCallback(BaseCallback):\n",
    "    def __init__(self, render_freq=1000, verbose=0, log_file='log.txt'):\n",
    "        super(RenderCallback, self).__init__(verbose)\n",
    "        self.render_freq = render_freq\n",
    "        self.log_file = log_file\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            render_output = self.training_env.render()\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(f\"Step: {self.n_calls}, Render Output: {render_output}\\n\")\n",
    "        return True\n",
    "# Initialize the environment\n",
    "\n",
    "# Create the SAC model\n",
    "model = SAC(\n",
    "    \"MlpPolicy\", enviorment, \n",
    "    learning_rate=3e-4,     \n",
    "    buffer_size=int(1e6), \n",
    "    batch_size=1024,       \n",
    "    tau=0.005,            \n",
    "    gamma=0.90,           \n",
    "    train_freq=(100 , \"step\"),         \n",
    "    gradient_steps=-1,     \n",
    "    ent_coef='auto',\n",
    "    verbose=1         # Enable logging\n",
    ")\n",
    "\n",
    "# Create a callback that renders the environment every 1000 steps\n",
    "render_callback = RenderCallback(render_freq=1000)\n",
    "\n",
    "# Train the model with the render callback\n",
    "model_SAC.learn(total_timesteps=1000000,progress_bar=False, callback=render_callback)\n",
    "model_SAC.save(\"SAC_Case 3 (REward changed).zip\")\n",
    "# model_SAC.learn(total_timesteps=500000,progress_bar=True)\n",
    "# model_SAC.save(\"SAC_Case 3 V2.zip\")\n",
    "# model_SAC.learn(total_timesteps=500000,progress_bar=True)\n",
    "# model_SAC.save(\"SAC_Case 3 V3.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [  3.65159255 532.85061054   0.        ] Total_Reward=-1000 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [  3.65159255 532.85061054   0.        ] Total_Reward=-1000 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [ 7.40328152 78.70300807  0.        ] Total_Reward=-2000 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [ 7.40328152 78.70300807  0.        ] Total_Reward=-2000 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [1.03954732e+01 6.67758417e+01 1.31886933e-03] Total_Reward=-2873.0437553032725 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [1.03954732e+01 6.67758417e+01 1.31886933e-03] Total_Reward=-2873.0437553032725 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [1.40076665e+01 5.64860169e+01 1.22869499e-02] Total_Reward=-2185.5206558228515 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [1.40076665e+01 5.64860169e+01 1.22869499e-02] Total_Reward=-2185.5206558228515 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [1.78975947e+01 4.33991851e+01 2.23957321e-02] Total_Reward=-445.579060277165 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [1.78975947e+01 4.33991851e+01 2.23957321e-02] Total_Reward=-445.579060277165 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [2.22273783e+01 3.76505040e+01 3.20360951e-02] Total_Reward=2271.3393856264074 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [2.22273783e+01 3.76505040e+01 3.20360951e-02] Total_Reward=2271.3393856264074 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [2.66928328e+01 4.73477027e+01 4.02826789e-02] Total_Reward=5891.030260334229 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [2.66928328e+01 4.73477027e+01 4.02826789e-02] Total_Reward=5891.030260334229 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [31.64216781 39.8258287   0.04948863] Total_Reward=10386.981542755448 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [31.64216781 39.8258287   0.04948863] Total_Reward=10386.981542755448 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [37.24667525 40.81319028  0.05973658] Total_Reward=15857.008430633294 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [37.24667525 40.81319028  0.05973658] Total_Reward=15857.008430633294 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current state: [43.03736197 40.61154404  0.06900284] Total_Reward=22285.987627422557 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Current state: [43.03736197 40.61154404  0.06900284] Total_Reward=22285.987627422557 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x23d91ca6990>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SAC.save(\"SAC_Case 3 V3.zip\")\n",
    "model_SAC.learn(total_timesteps=10000,callback=render_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#how about scalling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m obs \u001b[38;5;241m=\u001b[39m enviorment\u001b[38;5;241m.\u001b[39mreset()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model_SAC\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[1;32m----> 5\u001b[0m obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menviorment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Cell \u001b[1;32mIn[14], line 90\u001b[0m, in \u001b[0;36mNotreal.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate  , {}\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m#change self. state ?\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     92\u001b[0m     L, Fn \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenormalize_action(action)\n\u001b[0;32m     94\u001b[0m     cx,cn,cq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdernomalize_state( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards=[]\n",
    "for t in range(1000000):\n",
    "    obs = enviorment.reset()[0]\n",
    "    action, _states = model_SAC.predict(obs)\n",
    "    obs, reward, done, _, _ = enviorment.step(action)\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-413885"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class THIRD_ENV_WITH_DISTURBENCE(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(THIRD_ENV_WITH_DISTURBENCE, self).__init__()\n",
    "\n",
    "        # State space: concentrations of biomass (C_X), nitrate (C_N), and product (C_q)\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(3,), dtype=np.float32)\n",
    "        \n",
    "        # Action space: light intensity (I) and inflow rate (F_N)\n",
    "        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([400, 40]), dtype=np.float32)\n",
    "\n",
    "        # Initial states\n",
    "        self.state = None\n",
    "        self.time_step = 0\n",
    "        self.max_time_steps = 10  # Define the number of time steps in the batch\n",
    "\n",
    "        # Disturbance and noise parameters\n",
    "        self.sigma_d = np.array([4e-3, 1.0, 1e-7])  # \n",
    "        self.sigma_n = np.array([4e-4, 0.1, 1e-8])  \n",
    "\n",
    "        \n",
    "        self.penalty_coefficients = np.array([3.125e-8, 3.125e-6])\n",
    "\n",
    "    def reset(self):\n",
    "        # Initial concentrations (state)\n",
    "        self.state = np.array([1.0, 150.0, 0.0], dtype=np.float32)\n",
    "        self.time_step = 0\n",
    "        return self.state + np.random.normal(0, self.sigma_n)  # Add initial measurement noise\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply system dynamics and additive disturbance\n",
    "        I, F_N = action\n",
    "\n",
    "        C_X, C_N, C_q = self.state\n",
    "\n",
    "        dC_X = (I / (I + 178.9)) * C_X * (C_N / (C_N + 393.1)) - 0.0572 * C_X\n",
    "        dC_N = -504.1 * (I / (I + 178.9)) * C_X * (C_N / (C_N + 393.1)) + F_N\n",
    "        dC_q = (0.00016 * (I / (I + 23.51)) * C_X * (C_N / (C_N + 393.1))) if C_N <= 500 and C_X >= 10 else 0\n",
    "\n",
    "        # Add additive disturbances to the dynamics\n",
    "        disturbance = np.sin(self.time_step) * self.sigma_d + np.random.normal(0, self.sigma_d)\n",
    "        \n",
    "        dC_X += disturbance[0]\n",
    "        dC_N += disturbance[1]\n",
    "        dC_q += disturbance[2]\n",
    "\n",
    "        # Update state with dynamics and disturbances\n",
    "        self.state += np.array([dC_X, dC_N, dC_q])\n",
    "\n",
    "        # Add measurement noise when observing the state\n",
    "        noisy_state = self.state + np.random.normal(0, self.sigma_n)\n",
    "\n",
    "        if self.time_step < self.max_time_steps - 1:\n",
    "            reward = -np.dot(action - np.zeros(2), self.penalty_coefficients * (action - np.zeros(2)))\n",
    "        else:\n",
    "            reward = self.state[2]  # Final product concentration (C_q)\n",
    "\n",
    "        self.time_step += 1\n",
    "        done = self.time_step >= self.max_time_steps\n",
    "\n",
    "        return noisy_state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
