{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "class SDEEnv_train(gym.Env):\n",
    "    \"\"\" will remove the stoichastic part of the system \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SDEEnv_train, self).__init__()\n",
    "        # State is [y1, y2]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Actions are [u1, u2], both in some control range\n",
    "        self.action_space = spaces.Box(low=0, high=10, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Time step for numerical integration\n",
    "        self.dt = 0.001\n",
    "        \n",
    "        # Initial values for state variables y1 and y2\n",
    "        self.state = np.array([1, 1])  # You can set this based on the problem\n",
    "        \n",
    "    def reset(self,seed = None,options = None):\n",
    "        # Reset the state to initial values\n",
    "        self.state = np.array([0.1, 0.1]) #initial value can be changed, the bigger the value helps the model, but 0 is optimal\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        u1, u2 = action\n",
    "        y1, y2 = self.state\n",
    "        \n",
    "        dt = self.dt\n",
    "        \n",
    "        # Deterministic part of the system (first equation)\n",
    "        #dy1 = -(u1 + 0.5 * u1**2 * y1 + 0.5 * u2 * y2 / (y1 + y2)) * dt\n",
    "        dy1 = ( -1*(u1 + 0.5 * u1**2 )* y1 + 0.5 * u2 * y2 / (y1 + y2)) * dt\n",
    "\n",
    "        # Stochastic part of the second equation\n",
    "        dW = np.random.normal(0, np.sqrt(dt))  # Wiener process for stochastic term\n",
    "        dy2 = (u1 * y1 - 0.7 * u2 * y1) * dt + (0.1 * np.sqrt(y1)) \n",
    "        \n",
    "        \n",
    "        # Update states\n",
    "        y1 += dy1\n",
    "        y2 += dy2\n",
    "        \n",
    "        # Ensure non-negative concentrations\n",
    "        #y1 = max(0, y1)\n",
    "        #y2 = max(0, y2)\n",
    "        \n",
    "        self.state = np.array([y1, y2])\n",
    "        \n",
    "        # Reward is based on maximizing y2\n",
    "        reward = y2*10\n",
    "        \n",
    "        # Done if the system has run too long or if values go out of bounds\n",
    "        done = False\n",
    "        if y1 < .01 or y2 < .01:\n",
    "            reward = -1000\n",
    "            done = True\n",
    "        \n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # Optional rendering for visualization, not essential\n",
    "        print(f\"State: y1={self.state[0]}, y2={self.state[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SDEEnv_train_2(gym.Env):\n",
    "    \"\"\" nothing will change ,expcep that its stoichastic \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SDEEnv_train_2, self).__init__()\n",
    "        # State is [y1, y2]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Actions are [u1, u2], both in some control range\n",
    "        self.action_space = spaces.Box(low=0, high=10, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Time step for numerical integration\n",
    "        self.dt = 0.001\n",
    "        \n",
    "        # Initial values for state variables y1 and y2\n",
    "        self.state = np.array([1, 1])  \n",
    "        \n",
    "    def reset(self,seed = None,options = None):\n",
    "        # Reset the state to initial values\n",
    "        self.state = np.array([0.1, 0.1])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        u1, u2 = action\n",
    "        y1, y2 = self.state\n",
    "        \n",
    "        dt = self.dt\n",
    "        \n",
    "        # Deterministic part of the system (first equation)\n",
    "        dy1 = ( -1*(u1 + 0.5 * u1**2 )* y1 + 0.5 * u2 * y2 / (y1 + y2) ) * dt\n",
    "        \n",
    "        # Stochastic part of the second equation\n",
    "        dW = np.random.normal(0, np.sqrt(dt))  # Wiener process for stochastic term\n",
    "        dy2 = (u1 * y1 - 0.7 * u2 * y1) * dt + (0.1 * np.sqrt(y1) ) * dW\n",
    "        \n",
    "        \n",
    "        # Update states\n",
    "        y1 += dy1\n",
    "        y2 += dy2\n",
    "        \n",
    "        # Ensure non-negative concentrations\n",
    "        #y1 = max(0, y1)\n",
    "        #y2 = max(0, y2)\n",
    "        \n",
    "        self.state = np.array([y1, y2])\n",
    "        \n",
    "        # Reward is based on maximizing y2\n",
    "        reward = y2*100\n",
    "        \n",
    "        # Done if the system has run too long or if values go out of bounds\n",
    "        done = False\n",
    "        if y1 < 0 or y2 < 0:\n",
    "            reward = -1000\n",
    "            done = True\n",
    "        \n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # Optional rendering for visualization, not essential\n",
    "        print(f\"State: y1={self.state[0]}, y2={self.state[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=SDEEnv_train()\n",
    "env.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Environment representing the simplified model\n",
    "class PhotoProductionEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(PhotoProductionEnv, self).__init__()\n",
    "        \n",
    "        # Time step (10 intervals over a dimensionless time of 1)\n",
    "        self.dt = 0.1\n",
    "        self.n_steps = 10\n",
    "        \n",
    "        # Define state and action spaces\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=0, high=5, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Initial state values\n",
    "        self.state = np.array([1.0, 1.0])\n",
    "        self.t = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset states (random initial state)\n",
    "        self.state = np.array([0.1, 0.1])\n",
    "        self.t = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        u1, u2 = action\n",
    "        y1, y2 = self.state\n",
    "        \n",
    "        dt = self.dt\n",
    "        \n",
    "        # Equations (19) and (20)\n",
    "        dy1 = (-1 * (u1 + 0.5 * u1**2) * y1 + u2) * dt\n",
    "        dy2 = (u1 * y1 - u2 * y1) * dt\n",
    "        \n",
    "        # Add Gaussian noise with mean 0 and std deviation 0.02\n",
    "        noise = np.random.normal(0, 0.02, size=2)\n",
    "        y1 = max(0, y1 + dy1 + noise[0])\n",
    "        y2 = max(0, y2 + dy2 + noise[1])\n",
    "        \n",
    "        self.state = np.array([y1, y2])\n",
    "        self.t += 1\n",
    "        \n",
    "        # Reward is based on maximizing y2 at final time step\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if self.t >= self.n_steps:\n",
    "            reward = y2\n",
    "            done = True\n",
    "        \n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "# RNN Policy Network for each control action (u1 and u2)\n",
    "class RNNPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=20, output_dim=1):\n",
    "        super(RNNPolicyNetwork, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=2, nonlinearity='tanh', batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(2, batch_size, 20)\n",
    "\n",
    "# Train the RNN policies using REINFORCE algorithm\n",
    "def train_policy(env, n_epochs=100, n_episodes=800, lr=1e-2):\n",
    "    # Initialize policy networks for u1 and u2\n",
    "    policy_u1 = RNNPolicyNetwork()\n",
    "    policy_u2 = RNNPolicyNetwork()\n",
    "    \n",
    "    # Optimizers for both policies\n",
    "    optimizer_u1 = optim.Adam(policy_u1.parameters(), lr=lr)\n",
    "    optimizer_u2 = optim.Adam(policy_u2.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        total_rewards = []\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            hidden_u1 = policy_u1.init_hidden()\n",
    "            hidden_u2 = policy_u2.init_hidden()\n",
    "            \n",
    "            rewards = []\n",
    "            log_probs_u1 = []\n",
    "            log_probs_u2 = []\n",
    "            \n",
    "            for t in range(env.n_steps):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)  # Batch and time dimension\n",
    "                \n",
    "                # Policy for u1\n",
    "                action_u1, hidden_u1 = policy_u1(state_tensor, hidden_u1)\n",
    "                action_u1 = torch.clamp(action_u1, 0, 5)  # Constraint action to [0, 5]\n",
    "                dist_u1 = Normal(action_u1, 0.1)  # Action distribution with small std\n",
    "                u1 = dist_u1.sample()\n",
    "                log_prob_u1 = dist_u1.log_prob(u1)\n",
    "                \n",
    "                # Policy for u2\n",
    "                action_u2, hidden_u2 = policy_u2(state_tensor, hidden_u2)\n",
    "                action_u2 = torch.clamp(action_u2, 0, 5)\n",
    "                dist_u2 = Normal(action_u2, 0.1)\n",
    "                u2 = dist_u2.sample()\n",
    "                log_prob_u2 = dist_u2.log_prob(u2)\n",
    "                \n",
    "                # Step environment\n",
    "                action = [u1.item(), u2.item()]\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                log_probs_u1.append(log_prob_u1)\n",
    "                log_probs_u2.append(log_prob_u2)\n",
    "                \n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Compute returns (reward to go)\n",
    "            returns = []\n",
    "            R = 0\n",
    "            for r in reversed(rewards):\n",
    "                R = r + 0.99 * R  # Discount factor\n",
    "                returns.insert(0, R)\n",
    "            returns = torch.tensor(returns)\n",
    "            \n",
    "            # Update policy networks using REINFORCE\n",
    "            optimizer_u1.zero_grad()\n",
    "            optimizer_u2.zero_grad()\n",
    "            \n",
    "            loss_u1 = -torch.stack(log_probs_u1) * returns\n",
    "            loss_u2 = -torch.stack(log_probs_u2) * returns\n",
    "            \n",
    "            loss_u1.sum().backward()\n",
    "            loss_u2.sum().backward()\n",
    "            \n",
    "            optimizer_u1.step()\n",
    "            optimizer_u2.step()\n",
    "            \n",
    "            total_rewards.append(sum(rewards))\n",
    "        \n",
    "        # Average reward per epoch\n",
    "        print(f\"Epoch {epoch + 1}, Average Reward: {np.mean(total_rewards)}\")\n",
    "\n",
    "# Main Execution\n",
    "if _name_ == \"_main_\":\n",
    "    env = PhotoProductionEnv()\n",
    "    train_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
